
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Regularized Models in Action &#8212; A Gentle Introduction to Predictive Modelling</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="The Second Villain: Feature Leakage" href="leakage.html" />
    <link rel="prev" title="Feature Reduction and Regularization: a brief theory" href="theory_regularization.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QPT1100TCY"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-QPT1100TCY');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">A Gentle Introduction to Predictive Modelling</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_python_basics/index.html">
   1. Python Basics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_python_basics/python_intro.html">
     Introduction to python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_python_basics/data_frames.html">
     Working with DataFrames
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2_linear_models/index.html">
   2. Linear Models and Overfitting
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_models/theory_linear_models.html">
     Linear Models: a Brief Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_models/practice_linear_models.html">
     Linear Model in action
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_linear_models/overfitting_ex.html">
     The first Villain: overfitting
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3_cross_validation/index.html">
   3. Unbiased predictive performance estimates
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_cross_validation/train_test.html">
     Training and Test sets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_cross_validation/cv.html">
     Cross-validation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   4. Fighting Overfitting - The Advent of Machine Learning
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="theory_regularization.html">
     Feature Reduction and Regularization: a brief theory
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Regularized Models in Action
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="leakage.html">
     The Second Villain: Feature Leakage
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5_hyperparameter_optimization/index.html">
   5. Hyperparameter Optimization and Nested Cross-Validation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../6_validity/index.html">
   6. Validity, Generalizability, Fairness
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../7_model_explanation/index.html">
   7. Model Explanation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../8_complex_models/index.html">
   8. Complex Models, Deep Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../9_examples/index.html">
   9. Scientific Examples
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../references.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/4_reducing_complexity/practice_regularization.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/pni-lab/predmod_lecture"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/pni-lab/predmod_lecture/issues/new?title=Issue%20on%20page%20%2F4_reducing_complexity/practice_regularization.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/pni-lab/predmod_lecture/master?urlpath=tree/contents/4_reducing_complexity/practice_regularization.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/pni-lab/predmod_lecture/blob/master/contents/4_reducing_complexity/practice_regularization.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#manual-feature-selection">
   Manual feature selection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dimensionality-reduction-pca-and-pls">
   Dimensionality Reduction: PCA and PLS
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularized-models-ridge-and-lasso">
   Regularized models: Ridge and LASSO
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Regularized Models in Action</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#manual-feature-selection">
   Manual feature selection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dimensionality-reduction-pca-and-pls">
   Dimensionality Reduction: PCA and PLS
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularized-models-ridge-and-lasso">
   Regularized models: Ridge and LASSO
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="regularized-models-in-action">
<h1>Regularized Models in Action<a class="headerlink" href="#regularized-models-in-action" title="Permalink to this headline">¶</a></h1>
<p>Scikit-learn provides a wide variety of machine learning models that provide different mechanisms to control model complexity and implement different strategies to find the optimal model coefficients.</p>
<p>In this section we will apply scikit-learn to reduce model complexity by:</p>
<ul class="simple">
<li><p>manually dropping variables</p></li>
<li><p>PCA and PLS regression</p></li>
<li><p>Ridge and LASSO</p></li>
</ul>
<p>In this section, we will not optimize the hyperparameters, only select a single hyperparameter value.
In the <a class="reference internal" href="leakage.html"><span class="doc std std-doc">next section</span></a> we will demonstrate how selecting the hyperparameters can result in double-dipping, a.k.a. feature leakage.
In the <a class="reference internal" href="../5_hyperparameter_optimization/index.html"><span class="doc std std-doc">next chapter</span></a>, we will see how feature leakage during hyperparameter-tuning can be prevented by so-called nested cross-validation.</p>
<section id="manual-feature-selection">
<h2>Manual feature selection<a class="headerlink" href="#manual-feature-selection" title="Permalink to this headline">¶</a></h2>
<p>Here we do something we have already done <a class="reference internal" href="../2_linear_models/practice_linear_models.html"><span class="doc std std-doc">before</span></a>: we reduce model complexity by decreasing the number of predictors.</p>
<p>Below, we simply select the 5 best features (from of all 68) in an <em>independent dataset</em> and then use only these five features of the (<a class="reference internal" href="../3_cross_validation/cv.html"><span class="doc std std-doc">previously used</span></a>) training set to construct cross-validated estimates of predictive performance..</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#imports</span>
<span class="o">!</span>pip install scikit-learn &gt; /dev/null <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_predict</span><span class="p">,</span> <span class="n">KFold</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#load data</span>
<span class="n">df_full</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/pni-lab/predmod_lecture/master/ex_data/IXI/ixi.csv&quot;</span><span class="p">)</span>

<span class="c1"># select a subset for cross-validation</span>
<span class="n">df_cv</span> <span class="o">=</span> <span class="n">df_full</span><span class="o">.</span><span class="n">loc</span><span class="p">[:</span><span class="mi">100</span><span class="p">,</span> <span class="p">:]</span>
<span class="c1"># select an independent subset for searching for the k best features</span>
<span class="n">df_ext</span> <span class="o">=</span> <span class="n">df_full</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">100</span><span class="p">:</span><span class="mi">200</span><span class="p">,</span> <span class="p">:]</span>

<span class="c1"># names of the columns to be used</span>
<span class="n">target</span> <span class="o">=</span> <span class="s1">&#39;Age&#39;</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">df_cv</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>

<span class="c1"># select the 5 best features based on an independent sample</span>
<span class="n">k_best_selector</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">df_ext</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">X</span><span class="o">=</span><span class="n">df_ext</span><span class="p">[</span><span class="n">features</span><span class="p">])</span>
<span class="c1"># then we choose these 5 features from the training set (without looking at the target in the training set)</span>
<span class="n">k_best_features</span> <span class="o">=</span> <span class="n">k_best_selector</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_cv</span><span class="p">[</span><span class="n">features</span><span class="p">])</span>
<span class="c1"># cross validation</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># 5 folds, 20 participants in each fold</span>

<span class="n">cv_predictions</span> <span class="o">=</span> <span class="n">cross_val_predict</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">LinearRegression</span><span class="p">(),</span> <span class="n">y</span><span class="o">=</span><span class="n">df_cv</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">X</span><span class="o">=</span><span class="n">k_best_features</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">KFold</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_cv</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">cv_predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;MAE = &#39;</span><span class="p">,</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">df_cv</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">cv_predictions</span><span class="p">),</span> <span class="s1">&#39;years&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/tspisak/src/RPN-signature/venv/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:116: RuntimeWarning: divide by zero encountered in true_divide
  f = msb / msw
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MAE =  8.854781824723458 years
</pre></div>
</div>
<img alt="../_images/practice_regularization_3_2.png" src="../_images/practice_regularization_3_2.png" />
</div>
</div>
<p>Apparently, this results in a large improvement of cross-validated predictive performance, as compared to the example in the previous chapter, comprising all features.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Take a look at how scikit-learn’s <code class="docutils literal notranslate"><span class="pre">SelectKBest</span></code> feature selector is used. By calling the <code class="docutils literal notranslate"><span class="pre">fit</span></code> function, it selects the best features from the external independnet dataset and the calling the fitted selector’s <code class="docutils literal notranslate"><span class="pre">transform</span></code> function, it applies what it has “learned” from the extarnal dataset, i.e. chooses the “best features”.
The <code class="docutils literal notranslate"><span class="pre">fit</span></code>, <code class="docutils literal notranslate"><span class="pre">tarnsform</span></code>, and <code class="docutils literal notranslate"><span class="pre">fit_transform</span></code> (doing both operations on the same data) functions are central in scikit-learn, most of its functionality can be accessed trough these functions. Separating the learning process (<code class="docutils literal notranslate"><span class="pre">fit</span></code>) and the application of the learned knowledge (<code class="docutils literal notranslate"><span class="pre">transform</span></code>) always allows a clean separation of training and test datasets. We will see numerous examples to the usage of this scheme in the followings.</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Exercise 4.1</p>
<p>How much is the improvement exactly? Compare the above results to <a class="reference internal" href="../3_cross_validation/cv.html"><span class="doc std std-doc">the case</span></a> where we use all available features.</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Exercise 4.2</p>
<p>Try out different number of features. How does the cross-validated performance change?</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Exercise 4.3</p>
<p>Why do we have to use an <em>independent</em> dataset to select the k best features?
The answer is to be found in the <a class="reference internal" href="leakage.html"><span class="doc std std-doc">next section</span></a>.</p>
</div>
<p>Despite the improvement, intuitively, you might feel that we have thrown out some information of the window, as we might have included variables that - even tough weaker - but still significantly predict the target.</p>
<p>This issue can be tackled by converting the original features into new ‘latent’ variables, which contain the same information, but in a potentially more useful form.
That’s what PCA and PLS do!</p>
</section>
<section id="dimensionality-reduction-pca-and-pls">
<h2>Dimensionality Reduction: PCA and PLS<a class="headerlink" href="#dimensionality-reduction-pca-and-pls" title="Permalink to this headline">¶</a></h2>
<p>As discussed in the <a class="reference internal" href="theory_regularization.html"><span class="doc std std-doc">theory section</span></a>, with PCA we can transform our features into ‘latent’ variables that are possibly more useful than the original ones. With PCA, the “essence” of a group of highly correlated features is “condensed down” into a single variable. Thus, taking the principal components as predictive features often allows decreasing the complexity of the model to a higher degree, as fewer PCA variables will be able to provide the same explanatory power as a larger number of original features.</p>
<p>Let’s see it in practice! Below, we take the first five principal components calculated from all features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="n">features</span> <span class="o">=</span> <span class="n">df_cv</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">pca_features</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_cv</span><span class="p">[</span><span class="n">features</span><span class="p">])</span>

<span class="c1"># same as in the previous example except:                                          VVV here VVV</span>
<span class="n">cv_predictions</span> <span class="o">=</span> <span class="n">cross_val_predict</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">LinearRegression</span><span class="p">(),</span> <span class="n">y</span><span class="o">=</span><span class="n">df_cv</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">X</span><span class="o">=</span><span class="n">pca_features</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">KFold</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_cv</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">cv_predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;MAE = &#39;</span><span class="p">,</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">df_cv</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">cv_predictions</span><span class="p">),</span> <span class="s1">&#39;years&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MAE =  8.658108451519261 years
</pre></div>
</div>
<img alt="../_images/practice_regularization_6_1.png" src="../_images/practice_regularization_6_1.png" />
</div>
</div>
<p>Again a small improvement. And this time we didn’t have to use the independent external dataset, as - unlike <code class="docutils literal notranslate"><span class="pre">SelectKBest</span></code> - <code class="docutils literal notranslate"><span class="pre">PCA</span></code> does not incorporate any information about the target variable, only looks at the features.</p>
<p>Let’s look at PLS!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.cross_decomposition</span> <span class="kn">import</span> <span class="n">PLSRegression</span>

<span class="c1"># same as in the previous example except:            VVV here VVV</span>
<span class="n">cv_predictions</span> <span class="o">=</span> <span class="n">cross_val_predict</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">PLSRegression</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span> <span class="n">y</span><span class="o">=</span><span class="n">df_cv</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">X</span><span class="o">=</span><span class="n">df_cv</span><span class="p">[</span><span class="n">features</span><span class="p">],</span> <span class="n">cv</span><span class="o">=</span><span class="n">KFold</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_cv</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">cv_predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;MAE = &#39;</span><span class="p">,</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">df_cv</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">cv_predictions</span><span class="p">),</span> <span class="s1">&#39;years&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MAE =  8.190707243962333 years
</pre></div>
</div>
<img alt="../_images/practice_regularization_8_1.png" src="../_images/practice_regularization_8_1.png" />
</div>
</div>
<p>Performance slightly improved again. Note that in this case the dimensionality reduction and fitting the regression model happens in on e step, in <code class="docutils literal notranslate"><span class="pre">PLSRegression</span></code>.
Indeed, the power of PLS lies in tailoring the dimensionality reduction to the characteristics of the target variable.</p>
<p>Looks like with PCA and PLS we have a performance gain, as compared to using the original features. But do we also loose something?
Yes, interpretability. Checking out the next exercise may shed some light on this. More details will be given in <a class="reference internal" href="../7_model_explanation/index.html"><span class="doc std std-doc">chapter 7 “Model Explanation”</span></a></p>
<div class="tip dropdown admonition">
<p class="admonition-title">Exercise 4.4</p>
<p>Which regions were included in the prediction in case of SelectKBest (first example)? Printout their names!
Can you do the same with PCA or PLS?</p>
</div>
</section>
<section id="regularized-models-ridge-and-lasso">
<h2>Regularized models: Ridge and LASSO<a class="headerlink" href="#regularized-models-ridge-and-lasso" title="Permalink to this headline">¶</a></h2>
<p>Exercise 4.4 illustrates that transforming our original features to latent variables may have some disadvantages in terms of interpretability.
Let’s look at how regularized regression models can reduce model complexity (and, thereby, fight overfitting), but at the same time, retain all the original variables.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>

<span class="c1"># we taka all features</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">df_cv</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>

<span class="c1"># Changed:                                       VVV here VVV</span>
<span class="n">cv_predictions</span> <span class="o">=</span> <span class="n">cross_val_predict</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">10000000</span><span class="p">),</span> <span class="n">y</span><span class="o">=</span><span class="n">df_cv</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">X</span><span class="o">=</span><span class="n">df_cv</span><span class="p">[</span><span class="n">features</span><span class="p">],</span> <span class="n">cv</span><span class="o">=</span><span class="n">KFold</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_cv</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">cv_predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;MAE = &#39;</span><span class="p">,</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">df_cv</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">cv_predictions</span><span class="p">),</span> <span class="s1">&#39;years&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MAE =  8.00562203685471 years
</pre></div>
</div>
<img alt="../_images/practice_regularization_11_1.png" src="../_images/practice_regularization_11_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>

<span class="n">features</span> <span class="o">=</span> <span class="n">df_cv</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>

<span class="c1"># Changed:                                     VVV here VVV</span>
<span class="n">cv_predictions</span> <span class="o">=</span> <span class="n">cross_val_predict</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span> <span class="n">y</span><span class="o">=</span><span class="n">df_cv</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">X</span><span class="o">=</span><span class="n">df_cv</span><span class="p">[</span><span class="n">features</span><span class="p">],</span> <span class="n">cv</span><span class="o">=</span><span class="n">KFold</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_cv</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">cv_predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;MAE = &#39;</span><span class="p">,</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">df_cv</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">cv_predictions</span><span class="p">),</span> <span class="s1">&#39;years&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MAE =  8.028189984232535 years
</pre></div>
</div>
<img alt="../_images/practice_regularization_12_1.png" src="../_images/practice_regularization_12_1.png" />
</div>
</div>
<p>Both Ride and LASSO seem to be able to provide a predictive performance that is comparable (even slightly better), than using the dimension reduction-based approaches.</p>
<div class="tip dropdown admonition">
<p class="admonition-title">Exercise 4.5</p>
<p>How would other values of the alpha hyperparameter perform? Try it out!</p>
</div>
<p>Curious, which regions are driving the prediction?
Then we will now have a sneak-preview into model interpretation, which will be discussed in detail in <a class="reference internal" href="../7_model_explanation/index.html"><span class="doc std std-doc">chapter 7</span></a></p>
<p>Remember that Ridge and LASSO are just linear models with some extra steps.
So we can simply obtain the model coefficients (<span class="math notranslate nohighlight">\(\boldsymbol{b}\)</span>).
As the model’s prediction is just a weighted average of the feature values, where the weights are the model’s coefficients, the coefficients themselves can be considered as  a kind of ‘feature importance’: the degree to which that feature influences the predictions.</p>
<p>But there’s a problem: in case of cross-validation we have actually multiple model.</p>
<p>Therefore, we must <a class="reference internal" href="../3_cross_validation/cv.html"><span class="doc std std-doc">finalize</span></a> the model first.
Here we finalize our Ridge model by training it on the whole sample (without cv):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">df_cv</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">X</span><span class="o">=</span><span class="n">df_cv</span><span class="p">[</span><span class="n">features</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Ridge(alpha=1000)
</pre></div>
</div>
</div>
</div>
<p>And then, we can simply print out the coefficients, together with the names of the features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="n">features</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>lh_bankssts_volume</th>
      <td>0.000092</td>
    </tr>
    <tr>
      <th>lh_caudalanteriorcingulate_volume</th>
      <td>-0.001514</td>
    </tr>
    <tr>
      <th>lh_caudalmiddlefrontal_volume</th>
      <td>-0.001669</td>
    </tr>
    <tr>
      <th>lh_cuneus_volume</th>
      <td>-0.003873</td>
    </tr>
    <tr>
      <th>lh_entorhinal_volume</th>
      <td>-0.000299</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
    </tr>
    <tr>
      <th>rh_supramarginal_volume</th>
      <td>0.002263</td>
    </tr>
    <tr>
      <th>rh_frontalpole_volume</th>
      <td>-0.007186</td>
    </tr>
    <tr>
      <th>rh_temporalpole_volume</th>
      <td>0.006122</td>
    </tr>
    <tr>
      <th>rh_transversetemporal_volume</th>
      <td>-0.006611</td>
    </tr>
    <tr>
      <th>rh_insula_volume</th>
      <td>0.003572</td>
    </tr>
  </tbody>
</table>
<p>68 rows × 1 columns</p>
</div></div></div>
</div>
<p>With a few lines of code, we can visualize feature importance in 3D, to show the most important predictors of brain age:</p>
<div class="tip dropdown admonition">
<p class="admonition-title">Exercise 4.6</p>
<p>A little python exercise: sort the above list to see the regions with the smallest, largest coefficient.
Tip: look up the <a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html">pandas</a> documentation!
All you need is to add: <code class="docutils literal notranslate"><span class="pre">.sort_values(by=0)</span></code></p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Exercise 4.7</p>
<p>Sort the list of regions by the absolute value of their importance, to see the most important predictors in this model, regardless of the sign of the association with age.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip install ggseg &gt; /dev/null <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span>
<span class="kn">import</span> <span class="nn">ggseg</span>
<span class="n">features_plot</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="c1"># convert labels:</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
    <span class="n">parts</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">parts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;rh&#39;</span><span class="p">:</span>
        <span class="n">parts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;right&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">parts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;left&#39;</span>
    <span class="n">features_plot</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">parts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="n">parts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">ggseg</span><span class="o">.</span><span class="n">plot_dk</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">features_plot</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;bwr&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span>
              <span class="n">background</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">bordercolor</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span>
              <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Predictive Importance&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Morphological predictors of age&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/practice_regularization_19_0.png" src="../_images/practice_regularization_19_0.png" />
</div>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Exercise 4.8</p>
<p>Finalize the LASSO model, too, and print and plot the coefficients.
What is the main difference?</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Exercise 4.9</p>
<p>Load in your own dataset and see how these machine learning models work on that.</p>
</div>
<p>To recap, being able to control the complexity can drastically reduce the complexity of the model and thereby significantly improve predictive performance on unseen data.
The complexity of machine learning models can usually be set via one or more hyperparameters.</p>
<p>But how should we find the optimal values for the hyperparameters?
Should we simply do it in a trial and error fashion? Or should we systematically search for the value with which the cross-validated prediction look best?</p>
<p>Definitely not! To understand the reason why, we must get to know the second villain of machine learning: leakage!</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "pni-lab/predmod_lecture",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./4_reducing_complexity"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="theory_regularization.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Feature Reduction and Regularization: a brief theory</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="leakage.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">The Second Villain: Feature Leakage</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Tamas Spisak<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>